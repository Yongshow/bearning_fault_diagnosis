{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import preprocess\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from keras import Sequential\n",
    "from keras.layers import *\n",
    "from keras.utils import plot_model\n",
    "from attention import Attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练参数\n",
    "batch_size = 128\n",
    "epochs = 20\n",
    "num_classes = 10\n",
    "length = 2048\n",
    "BatchNorm = True # 是否批量归一化\n",
    "number = 1000 # 每类样本的数量\n",
    "normal = True # 是否标准化\n",
    "rate = [0.7,0.2,0.1] # 测试集验证集划分比例\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'data/0HP'\n",
    "x_train, y_train, x_valid, y_valid, x_test, y_test = preprocess.prepro(d_path=path,length=length,\n",
    "                                                                  number=number,\n",
    "                                                                  normal=normal,\n",
    "                                                                  rate=rate,\n",
    "                                                                  enc=True, enc_step=28)\n",
    "# 输入卷积的时候还需要修改一下，增加通道数目\n",
    "x_train, x_valid, x_test = x_train[:,:,np.newaxis], x_valid[:,:,np.newaxis], x_test[:,:,np.newaxis]\n",
    "# 输入数据的维度\n",
    "input_shape =x_train.shape[1:]\n",
    "\n",
    "print('训练样本维度:', x_train.shape)\n",
    "print(x_train.shape[0], '训练样本个数')\n",
    "print('验证样本的维度', x_valid.shape)\n",
    "print(x_valid.shape[0], '验证样本个数')\n",
    "print('测试样本的维度', x_test.shape)\n",
    "print(x_test.shape[0], '测试样本个数')\n",
    "input_shape,x_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建网络模型\n",
    "model = Sequential(\n",
    "    [\n",
    "        Conv1D(filters=16, kernel_size=32, strides=2, padding='same', activation='relu', input_shape=input_shape),\n",
    "        BatchNormalization(),\n",
    "        Conv1D(filters=32, kernel_size=3, strides=1, padding='same', activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling1D(pool_size=8),\n",
    "        Conv1D(filters=64, kernel_size=3, strides=1, padding='same', activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling1D(pool_size=4),\n",
    "        Bidirectional(GRU(units=128, return_sequences=True)),\n",
    "        Dropout(0.5),\n",
    "        Attention(256),\n",
    "        Dense(units=num_classes, activation='softmax')\n",
    "    ],name='mynn'\n",
    ")\n",
    "\n",
    "# 显示模型结构\n",
    "model.summary()\n",
    "# plot_model(model=model, to_file='mynn.png', show_shapes=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 编译模型 评价函数和损失函数相似，不过评价函数的结果不会用于训练过程中\n",
    "model.compile(optimizer='Adam', loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# 开始模型训练\n",
    "history = model.fit(x=x_train, y=y_train, batch_size=batch_size, epochs=epochs,\n",
    "          verbose=1, validation_data=(x_valid, y_valid), shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 评估模型\n",
    "score = model.evaluate(x=x_test, y=y_test)\n",
    "print(\"测试集上的损失：\", score[0])\n",
    "print(\"测试集上的精度:\",score[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_data = pd.DataFrame(history.history)\n",
    "run_data.plot(figsize=(9, 6))\n",
    "\n",
    "plt.grid(True, axis='y')\n",
    "plt.legend(loc='upper right')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('rate')\n",
    "plt.xticks(np.arange(0,21,2))\n",
    "plt.ylim(-0.01, 1.01)\n",
    "plt.savefig('fig_run.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型预测\n",
    "y_pred = model.predict(x_test)\n",
    "\n",
    "# 将one-hot编码转为整型编码\n",
    "y_test_int = np.array([np.argmax(i) for i in y_test])\n",
    "y_pred_int = np.array([np.argmax(i) for i in y_pred])\n",
    "\n",
    "# 创建混淆矩阵\n",
    "test_confu_matrix = confusion_matrix(y_test_int, y_pred_int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(9, 6))\n",
    "\n",
    "labels = ['12k_DE_B007_0', '12k_DE_B014_0', '12k_DE_B021_0', '12k_DE_IR007_0', '12k_DE_IR014_0',\n",
    "          '12k_DE_IR021_0', '12k_DE_OR007@6_0', '12k_DE_OR014@6_0', '12k_DE_OR021@6_0', 'normal_0']\n",
    "\n",
    "sns.heatmap(test_confu_matrix, annot = True, cmap = \"Blues\",  ax=ax)\n",
    "ax.set_xticklabels(labels)\n",
    "ax.set_yticklabels(labels)\n",
    "plt.xticks(rotation=45)\n",
    "plt.yticks(rotation=45)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.savefig('fig_confusion_matrix.png')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
